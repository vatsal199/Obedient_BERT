{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBYJ5j-wnm3M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "batch_size = 2\n",
    "sentence_len = 15\n",
    "max_iter = 500\n",
    "condition = \"really happy\"\n",
    "\n",
    "file_name = \"./generated_language.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqBXEM5Wax47"
   },
   "outputs": [],
   "source": [
    "out_file = file_name\n",
    "in_file = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UX-f2-RFXEqc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "\n",
    "# !pip install pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYFYuZAbnm3X",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    model = model.cuda(0)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ze0Bbo6Lnm3d"
   },
   "source": [
    "# Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LByxA6Yxnm3f"
   },
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    return idx.tolist() if return_list else idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Re2dz8Mdnm3l"
   },
   "outputs": [],
   "source": [
    "# Generation modes as functions\n",
    "\n",
    "def get_init_text(seed_text,inp_txt, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with condition and mask of lenght max_len \"\"\"\n",
    "    #batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "\n",
    "    batch = list()\n",
    "    inp_len = len(inp_txt)\n",
    "    seed_len = 1\n",
    "\n",
    "    # randomly introduce condition on initial sequence\n",
    "    rand_ind = np.random.randint(0, max_len - inp_len + 1)\n",
    "    for ind in range(batch_size):\n",
    "        temp = seed_text + [MASK] * rand_ind + inp_txt + [MASK] * (max_len - rand_ind - inp_len) + [SEP]\n",
    "        batch.append(temp)\n",
    "\n",
    "    return [tokenize_batch(batch),rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZ6iFT8bXgNW"
   },
   "outputs": [],
   "source": [
    "def getLeftInd(inp_ind,inp_len,max_len):\n",
    "    \"\"\" Generate indices that we can mask \"\"\"\n",
    "    ind_flag = [False]*max_len\n",
    "    for i in range(inp_len):\n",
    "        ind_flag[inp_ind + i] = True\n",
    "\n",
    "    left_ind = list()\n",
    "    for i in range(max_len):\n",
    "        if not ind_flag[i]:\n",
    "            left_ind.append(i)\n",
    "\n",
    "    return left_ind\n",
    "    left_len = len(left_ind)\n",
    "    kk = np.random.randint(0, left_len)\n",
    "    return left_ind[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtnMpLV_ZHTJ"
   },
   "outputs": [],
   "source": [
    "def getRandInd(left_ind):\n",
    "    \"\"\" left_ind contains indices of sentence except \"condition\". function will randomly choose index for mask. \"\"\"\n",
    "    left_len = len(left_ind)\n",
    "    kk = np.random.randint(0, left_len)\n",
    "    return left_ind[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ4fzt-zXeD9"
   },
   "outputs": [],
   "source": [
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "\n",
    "    inp_txt = seed_text[1:seed_len]\n",
    "    inp_len = seed_len - 1\n",
    "\n",
    "    seed_text = seed_text[0:1]\n",
    "    seed_len = 1\n",
    "\n",
    "    max_len = max_len + inp_len\n",
    "\n",
    "    noise_and_ind = get_init_text(seed_text,inp_txt, max_len, batch_size)\n",
    "    batch = noise_and_ind[0]\n",
    "    inp_ind = noise_and_ind[1]\n",
    "\n",
    "    left_ind = getLeftInd(inp_ind,inp_len,max_len)\n",
    "    for ii in range(max_iter):\n",
    "        kk = getRandInd(left_ind)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = mask_id\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        topk = top_k if (ii >= burnin) else 0\n",
    "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7PW2adMXZTw"
   },
   "outputs": [],
   "source": [
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    # main generation function to call\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
    "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                               cuda=cuda, verbose=False)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5E8p8Tzwnm3s"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n",
    "    \n",
    "def read_sents(in_file, should_detokenize=False):\n",
    "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
    "    if should_detokenize:\n",
    "        sents = [detokenize(sent) for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def write_sents(out_file, sents, should_detokenize=False):\n",
    "    with open(out_file, \"w\") as out_fh:\n",
    "        for sent in sents:\n",
    "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
    "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "v23k97u6nm3z",
    "outputId": "89210909-f448-46e2-a511-57f7d9814d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished batch 1 in 133.232s\n",
      "Finished batch 2 in 132.599s\n",
      "Finished batch 3 in 133.154s\n",
      "Finished batch 4 in 132.877s\n",
      "Finished batch 5 in 133.343s\n",
      "Finished batch 6 in 133.594s\n",
      "Finished batch 7 in 133.783s\n",
      "Finished batch 8 in 134.045s\n",
      "Finished batch 9 in 133.256s\n",
      "Finished batch 10 in 133.574s\n"
     ]
    }
   ],
   "source": [
    "max_len = sentence_len\n",
    "\n",
    "top_k = 100\n",
    "temperature = 0.7\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 250\n",
    "sample = True\n",
    "\n",
    "\n",
    "# Choose the prefix context\n",
    "in_text = \"[CLS]\"+\" \"+condition\n",
    "seed_text = in_text.split()\n",
    "\n",
    "for temp in [1.0]:\n",
    "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
    "                          cuda=False)\n",
    "                              \n",
    "    write_sents(out_file, bert_sents, should_detokenize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "1fBzgtLinm35",
    "outputId": "8b59e1b7-5842-47dd-a301-3507cf224667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was really happy about the change . i had never been to this festival before .\n",
      "they look really happy now , too , some really happy , but mostly not happy .\n",
      "\" oh hello , madaug ! i am really happy just having you back .\n",
      "and if there was anything to ask , i was really happy to see him tonight .\n",
      "that boy really loved me . then there were two . and i was really happy .\n",
      "that was when my parents hugged me . oh god , i was just really happy .\n",
      "the painting presents white silhouettes to symbolize how there will be really happy marriages .\n",
      "pity i cannot keep my eyes off him , but he ... looks really happy inside .\n",
      "\" really happy to see you again . \" sean loved the sound of the bear .\n",
      "\" really happy , \" he muttered . \" so happy . so damn happy . \"\n",
      "not really happy either , although i hear it a lot . he checks his watch .\n",
      "never really happy with herself , she was married to a man from the far east .\n",
      "opening the door i exclaim , \" how are you looking ? \" really happy .\n",
      "by the minute , you can see videos in high definition of people being really happy .\n",
      "\" your mom is really kind , \" my mom said . she sounded really happy .\n",
      "embarrassed , i thought back to north central . mom and dad had been really happy .\n",
      "\" that was my daddy . \" jeff said , admitting they were really happy together .\n",
      "like this was a sign that we were both really , really , really happy together .\n",
      "it was more happy than anything else , and i was actually really happy about that .\n",
      "\" i still think the same thing . \" he seemed ... really happy , actually .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_sents = read_sents(in_file, should_detokenize=False)\n",
    "for i in range(min(n_samples,50)):\n",
    "    printer(bert_sents[i], should_detokenize=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert-babble.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
